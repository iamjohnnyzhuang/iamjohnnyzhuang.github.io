---
layout: post
title: JStorm集群外部统一配置
categories: [big data]
---

# 前言

我们的Jstorm集群之前的一些配置信息（主要是数据连接信息）是写在和作业Jar包里面resource目录下的一个properties文件。这样子的做法很不利于后期统一修改，每一次要修改一点东西都需要经历繁琐的步骤：

1. 修改代码文件
2. 上传到git仓库
3. 通过jenkins重新打包
4. 手动到JStorm线上集群kill掉原来的任务，重新运行。 （jenkins没有集成kill以及start操作）

虽然数据库可以做域名来减少重复打包的操作，但是依然没办法解决的一个问题就是以上的4这个步骤，因为缺少了热部署的特性。

因此开始考虑一些新的集群配置方式。在预研的过程中也发现了不少好的方法，在此做下记录。



# 准备工作

既然要做的是JStorm的改进，那么必须得先了解下对JStorm的jar包运行了命令到底是怎么处理参数的。



``` bash
jstorm jar /usr/local/topology/mywork.jar 
```



运行了JStorm的任务提交命令后，在构造topology的时候会把我们设定在配置文件里的参数读取出来然后设置到对应的bolt、spout的成员变量里面，然后将bolt以及spout都序列化后传输到集群中的某台机器中，在worker中再反序列化出来。

因此worker并不在乎说配置文件是否可以在它运行的时候或者dead重新部署的时候读到配置。所以配置文件是否嵌入在Jar包是没有影响的。

好了，知道这个概念可以开始想设计方案了！



# 具体实现

## 1. 分布式文件配置实现方式

既然Worker的配置仅仅依赖于在nimbus运行时读入的配置信息，那么我们完全可以剥离掉jar包中的配置信息统一放在nimbus服务器上的某个目录下如： /etc/topology/

这样就可以很轻松地把配置文件提取出来了，但是依然不能热部署，我要改个参数虽然不用重新打包了依然需要restart。

考虑到热部署，我们可以在我们的代码中控制做个**定时读文件**操作，然后更新配置信息。但是这就意味着我就必须得在集群每台机器都放置同样的配置文件了。且必须保证他们的修改是同步的。

Linux下有很多文件同步的工具，可以使用这些工具来保证一个地方修改了文件其他地方的也跟随着修改。
或许也可以引入NFS，Nimbus服务器作为NFS服务器其他的读配置文件都从Nimbus机器去读。（这个未验证，不过感觉应该可行）

### 方案总结

* 优点： 优点很明显这个方案相对其他的来说比较简单但说的难听的就是暴力。而且配置信息都在配置文件里面修改起来也比较简单，底层工具搭建好了可以自动实现同步无需我们再去操劳。
* 缺点：缺点也非常的明显，首先依赖于同步工具，如果修改了Nimbus的配置信息同步工具由于网络或者其他不可知的原因导致没有同步过去，或者同步延迟那么就会造成配置没有生效或者延迟生效。其次是很丑，真的很丑。




## 2. 数据库配置方式

我们的项目中现在每个集群都有一个自己的基础数据库，它的数据库信息我们写在**环境变量**里面，因此一个作业运行了，它肯定会有的是这个基础数据库配置信息。

那么我们可以通过这个数据库做什么呢？我们可以在数据库上写每个作业的配置参数，包括数据库地址、业务的参数等等。
然后客户端代码定时从数据库拉取配置信息更新bolt、spout的配置值。

### 方案总结

优点：这个方案是比较可行的，由于数据库是建立在集群中的因此延迟非常低，而且如果单单是配置信息数据量并不会大，由于数据库大量读写造成的数据库性能低下、锁表等问题出现的概率也就比较低了。其次由于是数据库定期读取因此可以很轻松的进行配置的热部署。

缺点：这个方案的主要缺点在于依赖于数据库，JStorm本身是无需以来本地数据库的，但是为了做这个配置信息我们得在集群中特意搭建一个数据库，这难免是对性能的浪费。其次毕竟还是依赖于数据库如果出现人为误操作，或者某种原因导致数据库不可用，那么所有的任务都将会受到影响。



## 3. Zookeeper配置方式

记得刚跟一同事说这方案的时候他的第一反应是：就做这么个改进还要引入ZK啊。 
我马上跟他说，不是引入ZK，是**JStorm本身就有ZK，我们要做的只是利用好这个ZK。**

嗯对，这个方案最诱人的就是我们不必再像数据库那样特意去搭建一个，而是引用JStorm必须要以来的这个Zk。
而且不用单独考虑ZK的稳定性，如果这个ZK不稳定了，JStorm集群运行必然有问题。因此可以保证的是，如果JStorm运行正常，ZK就正常，我们的改进方案也就运行正常。

具体方案实现如图所示：

![](https://iamjohnnyzhuang.github.io/public/upload/7.png)

1. 首先我们的配置文件放置在nimbus的一个指定位置如：/etc/topo下面，该目录下放置每一个topology具体的配置
2. 每个Worker都Watch自己的ZK节点，如果收到通知（Hash值变化了）就读取新的properties进行业务的更新
3. 在Nimbus上运行两个脚本，
* Nimbus文件扫描脚本会每分钟去扫描整个目录，然后对每个topology的文件求hash值或者md5值（只要文件没被修改过那么hash/md5必然不会变），如果发现hash值和原来的不同则去ZK获取该topology的值。如果和zk上存储的该topology值不同则说明该配置文件被修改过了，修改ZK上的PROPERTIES值（将配置文件信息序列化成JSON字符串），然后修改ZK的HASH值触发WORKER的监听。最后修改本地内存对该topology的HASH值缓存。
* Nimbus域名扫描脚本会定时的扫描配置文件中的数据库域名信息，然后通过调用系统dig方法计算出该域名指定IP地址与原来是否一样（原来的值缓存在内存中），如果发现已经切换过了，那么将该对应topology的ZK HASH节点值切换为DOMAIN_CHANGE。Worker那边发现ZK节点变化的值即可得知域名的切换。

### 方案总结
优点：及时性更高，通过ZK实现了“推”的形式而不再是客户端主动去拉。其次JStorm本身需要ZK无需再重复创建一个ZK集群，可靠性也得到保证。同时基于ZK的设计更加严谨，结构也更加优美。

缺点：较为繁琐，还得在Nimbus特意跑两个脚本。


# 写在最后

以上讲述了三种方法，其中第一种方法实现简单，但是不稳定，第二种方法使用方便，但是需要额外创建数据库，第三种更为全面，但是部署、管理更为繁琐。

总而言之，方案还是需要根据具体的业务选择。

因为对于我们的业务来说我们将来需要实现很多配置不仅仅是简单的提取出jar包更是实现页面可配。如果要页面可配那么显然是选择第二种方案更佳。





g