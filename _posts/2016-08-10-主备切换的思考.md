---
layout: post
title: 主备热切换的思考
categories: [architecture]
---

# 前言

公司的项目为了构建高可用（HA）必须搭建主备切换机制。即有一台主机以及一台备用机，当主机出现问题（项目分解为三个服务运行在同一台机器上，任何一个出现问题则算出现问题）时必须得马上切换到备份机。因为两台机器的数据几乎实现了强同步，因此切换最主要要做的只是简单的切换DNS。

比较头疼的是，项目主备部署在异地机房。一开始采用的是Zookeeper机制，即主机连接上Zk后，创建三个临时节点（对应三个服务）当Watch到任何一个节点消失了，则动态调用切换DNS接口完成主备切换。但是异地Zk集群也有很多不稳定因素，时长会出现MISS事件的情况，即Watch并没有推送过来。因此，我们开始考虑用其他的现成技术解决。



---



## 方案一. 改进Zookeeper——Curator

一开始，我还是比较坚持使用Zookeeper的，因为毕竟Zk是一个比较成熟的项目，我们今天遇上的问题，别人肯定也有遇上过。然后经过一番调研发现，网上对Zk的评价并不是非常友好，比如Apache的开源项目Curator这样列举了Zk的缺点：



> 初始化连接的问题: 在client与server之间握手建立连接的过程中, 如果握手失败, 执行所有的同步方法(比如create, getData等)将抛出异常 
>
> 自动恢复(failover)的问题: 当client与一台server的连接丢失,并试图去连接另外一台server时, client将回到初始连接模式 
>
> session过期的问题: 在极端情况下, 出现ZooKeeper session过期, 客户端需要自己去监听该状态并重新创建ZooKeeper实例 . 
>
> 对可恢复异常的处理:当在server端创建一个有序ZNode, 而在将节点名返回给客户端时崩溃, 此时client端抛出可恢复的异常, 用户需要自己捕获这些异常并进行重试 
>
> 使用场景的问题:Zookeeper提供了一些标准的使用场景支持, 但是ZooKeeper对这些功能的使用说明文档很少, 而且很容易用错. 在一些极端场景下如何处理, zk并没有给出详细的文档说明. 比如共享锁服务, 当服务器端创建临时顺序节点成功, 但是在客户端接收到节点名之前挂掉了, 如果不能很好的处理这种情况, 将导致死锁. 



而Curator所做的改进：



> 重试机制:提供可插拔的重试机制, 它将给捕获所有可恢复的异常配置一个重试策略, 并且内部也提供了几种标准的重试策略(比如指数补偿). 
>
> 连接状态监控: Curator初始化之后会一直的对zk连接进行监听, 一旦发现连接状态发生变化, 将作出相应的处理. 
>
> zk客户端实例管理:Curator对zk客户端到server集群连接进行管理. 并在需要的情况, 重建zk实例, 保证与zk集群的可靠连接 
>
> 各种使用场景支持:Curator实现zk支持的大部分使用场景支持(甚至包括zk自身不支持的场景), 这些实现都遵循了zk的最佳实践, 并考虑了各种极端情况. 



总的来说在可用性方面已经提升了很多。于是就考虑是否能用Curator来优化异地主备切换的（**解决是不可能的，引起事件丢失的是不可靠的网络，所以能做的仅仅是优化**）。

只是后来，听公司的人说，其他组件有的有在使用Curator但是也有一些问题，于是，对于第一个解决方案就停留在了理论阶段。





## 方案二. 使用Keepalived

既然不能使用Zk了，所以我就去google寻找可行的其他方案。结果发现了这个东西——Keepalived。用于构建高可用的架构。



> Keepalived的作用是检测服务器的状态，如果有一台web服务器死机，或工作出现故障，Keepalived将检测到，并将有故障的服务器从系统中剔除，当服务器工作正常后Keepalived自动将服务器加入到服务器群中，这些工作全部自动完成，不需要人工干涉，需要人工做的只是修复故障的服务器。
>
> 

Keepalived的原理很简单，首先我们得先理解一下LVS的含义：



> LVS是Linux Virtual Server的简写，意即Linux虚拟服务器，是一个虚拟的服务器集群系统。
>
> 

网上找了张图：

![Alt text](https://iamjohnnyzhuang.github.io/public/upload/6.png)


如图LVS的意思就是说用户通过一个虚拟的IP访问我们的集群得到服务，但是集群对用户而言是透明的。

比如图中测试机就是用户，访问中间的虚拟ip（假如是192.168.10.20），对用户而言他以为我们的项目地址就是192.168.10.20然而这只是我们集群的一个调度服务器，它接受到用户的请求然后将其调度到正确的服务器上去。

那么keepalived是怎么工作的呢？在主服务器和备用服务器上分别运行着keepalived服务，备用服务器会检测主服务器keepalived服务运行的状况，如果主服务器的keepalived服务被干掉了，那么虚拟服务器就会将请求转发到备用服务器，这个过程被称作ip漂移。

所以，对于我们一台主机运行多个组件的情况下要想利用keepalived。要写一个监测脚本，实时观察当前服务器运行的组件，如果有任何一个挂掉了，那么直接killall keepalived。即可完成ip漂移。

**但是这个方案还是不适用于我们的项目，因为我们的项目是跨机房的。而Keeplived只能用于LVS中，LVS模式要求VIP（虚拟IP）和RIP（真实IP）在同一网段，因此对于我们的项目，这个方案也不行。**





## 方案三. 使用Heartbeat

> Heartbeat 项目是 Linux-HA 工程的一个组成部分，它实现了一个高可用集群系统。心跳服务和集群通信是高可用集群的两个关键组件，在 Heartbeat 项目里，由 heartbeat 模块实现了这两个功能。



Heartbeat也可以用于构建高可用集群。并且相比Keepalived它更复杂，更强大。Heartbeat并没有要求一定要使用LVS，Heartbeat要求至少两台主机一台主机发送心跳一台备用机接受心跳即可。

关于Heartbeat的主备切换工作原理如下：



> 通过修改Heartbeat的配置文件，可以指定哪台Heartbeat服务器作为主服务器，则另一台服务器自动成为热备服务器，然后在热备服务器上配置Heartbeat守护程序来监听来自主服务器的心跳消息。如果热备服务器在指定的时间内未监听到来自主服务器的心跳，就会启动故障转移程序，并取得主服务器上的相关资源服务的所有权，接替主服务器继续不间断的提供服务，从而达到资源及服务高可用性的目的。



看似貌似可以支持异地机房的情况，但是问题在于两台主机之间是如何做到互相通信和互相监测的呢？

  下面是两台heartbeat主机之间通信的一些常用可行方法：
* 利用串行电缆，即所谓的串口线连接两台服务器
* 一根以太网电缆两网卡直连
   * 以太网电缆，通过交换机等网络设备连接（不推荐）

**显然，要保证心跳的稳定性如果依赖于不可靠的网络，那么无异于直接使用zk！所以这个方案还是放弃。**



---





## 写在最后

是的，到最后还是没有找到直接通过软件就可以解决跨机房主备切换的问题。

调查了很多种方法，从最简单的直接写监控脚本然后以来HTTP请求方法到各种成熟框架服务，发现方法很多，但是**解决跨机房**问题的真的好少，几乎是看不到的。现成公司的解决方案大部分是在同城中分布主备机，异地机器用于冷备等需求。但是跨机房的存在也不是无理的，毕竟这种异地的容灾才能把地震、机房断电等不可控因素压缩到极致，将容灾做到极致。

所以唯一能解决的我想应该是从硬件角度出发：采用的是异地专线让网络更加的可靠稳定。或者直接改造构成LVS。

**如果有谁知道纯软件解决方案，请一定告知我！**



---



## 参考资料

1. [Nginx+Keepalived主备切换](http://blog.sina.com.cn/s/blog_79ac6aa80101bmed.html)
2. [Heartbeat工作原理及其部署需求](http://www.pualinux.com/cluster-ha/heartbeat-principle.html)

